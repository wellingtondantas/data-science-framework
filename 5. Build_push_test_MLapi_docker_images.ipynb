{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerize model server & ML API with Docker\n",
    "\n",
    "### Build and push docker images to docker-repository\n",
    "One common way of distributing this model API server for production deployment, is via Docker containers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod 777 /var/run//docker.sock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.354MB\n",
      "Step 1/5 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> e2f19ac0b4e3\n",
      "Step 2/5 : COPY /app/requirements.txt /app/\n",
      " ---> Using cache\n",
      " ---> 9e361e5ae998\n",
      "Step 3/5 : RUN pip install -r /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> aeb9ca75bf7f\n",
      "Step 4/5 : COPY ./model /model/\n",
      " ---> Using cache\n",
      " ---> a99fc343dab0\n",
      "Step 5/5 : COPY ./app /app\n",
      " ---> Using cache\n",
      " ---> 7b6a67337a31\n",
      "Successfully built 7b6a67337a31\n",
      "Successfully tagged kumarvc/fastapi-placement-classifier:1.0.3\n",
      "The push refers to repository [docker.io/kumarvc/fastapi-placement-classifier]\n",
      "\n",
      "\u001b[1Bda0cc980: Preparing \n",
      "\u001b[1B84f549ea: Preparing \n",
      "\u001b[1B8885b7d9: Preparing \n",
      "\u001b[1B949c3b04: Preparing \n",
      "\u001b[1B2331eddf: Preparing \n",
      "\u001b[1Bdd42a306: Preparing \n",
      "\u001b[1B5e087746: Preparing \n",
      "\u001b[1Ba89f95f7: Preparing \n",
      "\u001b[1B960321f5: Preparing \n",
      "\u001b[1Bc8cc20a5: Preparing \n",
      "\u001b[1B85a516c9: Preparing \n",
      "\u001b[1Bab020550: Preparing \n",
      "\u001b[1B5ea49213: Preparing \n",
      "\u001b[1B28316107: Preparing \n",
      "\u001b[1B0ec29f78: Preparing \n",
      "\u001b[1Bcecc2826: Preparing \n",
      "\u001b[1B81fca4b7: Preparing \n",
      "\u001b[1B92e98337: Preparing \n",
      "\u001b[1B306e673e: Preparing \n",
      "\u001b[1Ba3b3ed45: Preparing \n",
      "\u001b[1Ba51ade6a: Preparing \n",
      "\u001b[1B297cf5f8: Layer already exists 12A\u001b[2K\u001b[18A\u001b[2K\u001b[22A\u001b[2K\u001b[19A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[17A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K1.0.3: digest: sha256:a613bc9dd0837701b059f9959d5404aed46948a1783097e0c1f0ea287ba628e2 size: 4927\n"
     ]
    }
   ],
   "source": [
    "# Replace {docker_username} with your Docker Hub username\n",
    "docker_username = 'kumarvc'\n",
    "# Build and tage docker images\n",
    "!docker build -t {docker_username}/fastapi-placement-classifier:1.0.3 .\n",
    "# Push Docker images to Docker hub (Public repository)\n",
    "!docker push {docker_username}/fastapi-placement-classifier:1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run containerize model server - Ready to receive request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71ba710def3b1891cda3116e7220554b19241522bae3520dbf29f36129d3490a\n"
     ]
    }
   ],
   "source": [
    "# For full documentation please refere https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker\n",
    "!docker run --name fastapi-server -d -p 80:80 -e WORKERS_PER_CORE=\"3\" {docker_username}/fastapi-placement-classifier:1.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to Local FastAPI model server\n",
    "\n",
    "The Fast API server also provides a simple web UI dashboard (Swagger). Go to http://0.0.0.0:80/docs in the browser and use the Web UI to send prediction request:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Batch of test request from Python program or from python command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[\"Placed\"]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "data = {\n",
    "  \"sl_no\": 112,\n",
    "  \"ssc_p\": 84.0,\n",
    "  \"hsc_p\": 90.9,\n",
    "  \"degree_p\": 64.5,\n",
    "  \"etest_p\": 86.04,\n",
    "  \"mba_p\": 59.42,\n",
    "  \"gender\": \"M\",\n",
    "  \"ssc_b\": \"Others\",\n",
    "  \"hsc_b\": \"Others\",\n",
    "  \"hsc_s\": \"Science\",\n",
    "  \"degree_t\": \"Sci&Tech\",\n",
    "  \"workex\": \"No\",\n",
    "  \"specialisation\": \"Mkt&Fin\"\n",
    "}\n",
    "response = requests.post(\"http://0.0.0.0:80/predict\", json=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\":[\"Not Placed\"]}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"sl_no\": 113,\n",
    "  \"ssc_p\": 52.0,\n",
    "  \"hsc_p\": 57.0,\n",
    "  \"degree_p\": 50.8,\n",
    "  \"etest_p\": 67.0,\n",
    "  \"mba_p\": 62.79,\n",
    "  \"gender\": \"M\",\n",
    "  \"ssc_b\": \"Central\",\n",
    "  \"hsc_b\": \"Central\",\n",
    "  \"hsc_s\": \"Commerce\",\n",
    "  \"degree_t\": \"Comm&Mgmt\",\n",
    "  \"workex\": \"No\",\n",
    "  \"specialisation\": \"Mkt&HR\"\n",
    "}\n",
    "response = requests.post(\"http://0.0.0.0:80/predict\", json=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Docker logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for script in /app/prestart.sh\n",
      "Running script /app/prestart.sh\n",
      "Running inside /app/prestart.sh, you could add migrations to this file, e.g.:\n",
      "\n",
      "#! /usr/bin/env bash\n",
      "\n",
      "# Let the DB start\n",
      "sleep 10;\n",
      "# Run migrations\n",
      "alembic upgrade head\n",
      "\n",
      "[2021-01-03 12:25:31 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2021-01-03 12:25:31 +0000] [1] [INFO] Listening at: http://0.0.0.0:80 (1)\n",
      "[2021-01-03 12:25:31 +0000] [1] [INFO] Using worker: uvicorn.workers.UvicornWorker\n",
      "[2021-01-03 12:25:31 +0000] [8] [INFO] Booting worker with pid: 8\n",
      "[2021-01-03 12:25:31 +0000] [9] [INFO] Booting worker with pid: 9\n",
      "[2021-01-03 12:25:31 +0000] [10] [INFO] Booting worker with pid: 10\n",
      "[2021-01-03 12:25:31 +0000] [11] [INFO] Booting worker with pid: 11\n",
      "[2021-01-03 12:25:32 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2021-01-03 12:25:32 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "[2021-01-03 12:25:32 +0000] [14] [INFO] Booting worker with pid: 14\n",
      "[2021-01-03 12:25:32 +0000] [15] [INFO] Booting worker with pid: 15\n",
      "[2021-01-03 12:25:32 +0000] [16] [INFO] Booting worker with pid: 16\n",
      "[2021-01-03 12:25:54 +0000] [10] [INFO] Started server process [10]\n",
      "[2021-01-03 12:25:54 +0000] [8] [INFO] Started server process [8]\n",
      "[2021-01-03 12:25:54 +0000] [8] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [10] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [10] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [8] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [13] [INFO] Started server process [13]\n",
      "[2021-01-03 12:25:54 +0000] [13] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [13] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [11] [INFO] Started server process [11]\n",
      "[2021-01-03 12:25:54 +0000] [11] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [11] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [12] [INFO] Started server process [12]\n",
      "[2021-01-03 12:25:54 +0000] [12] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [12] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [9] [INFO] Started server process [9]\n",
      "[2021-01-03 12:25:54 +0000] [9] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [9] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [16] [INFO] Started server process [16]\n",
      "[2021-01-03 12:25:54 +0000] [16] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [16] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [14] [INFO] Started server process [14]\n",
      "[2021-01-03 12:25:54 +0000] [14] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [14] [INFO] Application startup complete.\n",
      "[2021-01-03 12:25:54 +0000] [15] [INFO] Started server process [15]\n",
      "[2021-01-03 12:25:54 +0000] [15] [INFO] Waiting for application startup.\n",
      "[2021-01-03 12:25:54 +0000] [15] [INFO] Application startup complete.\n",
      "{\"loglevel\": \"info\", \"workers\": 9, \"bind\": \"0.0.0.0:80\", \"graceful_timeout\": 120, \"timeout\": 120, \"keepalive\": 5, \"errorlog\": \"-\", \"accesslog\": \"-\", \"workers_per_core\": 3.0, \"use_max_workers\": null, \"host\": \"0.0.0.0\", \"port\": \"80\"}\n",
      "172.17.0.1:49500 - \"POST /predict HTTP/1.1\" 200\n",
      "{\"loglevel\": \"info\", \"workers\": 9, \"bind\": \"0.0.0.0:80\", \"graceful_timeout\": 120, \"timeout\": 120, \"keepalive\": 5, \"errorlog\": \"-\", \"accesslog\": \"-\", \"workers_per_core\": 3.0, \"use_max_workers\": null, \"host\": \"0.0.0.0\", \"port\": \"80\"}\n",
      "172.17.0.1:49816 - \"POST /predict HTTP/1.1\" 200\n"
     ]
    }
   ],
   "source": [
    "!docker logs 1ff1aae77f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and delete the running MLAPI container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ff1aae77f61ca43317\n",
      "1ff1aae77f61ca43317\n"
     ]
    }
   ],
   "source": [
    "!docker stop 1ff1aae77f61ca43317\n",
    "!docker rm 1ff1aae77f61ca43317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi-kernel",
   "language": "python",
   "name": "fastapi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
